{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_starting_probabilities(states):\n",
    "    return np.full(states, 1/states)\n",
    "\n",
    "def generate_transition_matrix(states, stay_prob=0.95):\n",
    "    # Create an empty transition matrix\n",
    "    transition_matrix = np.zeros((states, states))\n",
    "    change_prob = 1 - stay_prob\n",
    "\n",
    "    # Set the probabilities for the first and last states\n",
    "    transition_matrix[0, 0] = stay_prob\n",
    "    transition_matrix[0, 1] = change_prob\n",
    "    transition_matrix[states-1, states-1] = stay_prob\n",
    "    transition_matrix[states-1, states-2] = change_prob\n",
    "\n",
    "    # Set the probabilities for the middle states\n",
    "    for i in range(1, states-1):\n",
    "        transition_matrix[i, i] = stay_prob\n",
    "        transition_matrix[i, i-1] = change_prob/2\n",
    "        transition_matrix[i, i+1] = change_prob/2\n",
    "\n",
    "    # Normalize the rows of the transition matrix\n",
    "    row_sums = transition_matrix.sum(axis=1)\n",
    "    transition_matrix = transition_matrix / row_sums[:, np.newaxis]\n",
    "\n",
    "    return transition_matrix\n",
    "\n",
    "def generate_emission_probabilities(states, outputs):\n",
    "    emission_probabilities = np.zeros((states, outputs))\n",
    "\n",
    "    # Calculate the center of the distribution for each state\n",
    "    centers = np.linspace(0, outputs - 1, num=states)\n",
    "\n",
    "    for i in range(states):\n",
    "        center = centers[i]\n",
    "        for j in range(outputs):\n",
    "            distance = np.abs(j - center)\n",
    "\n",
    "            # Use a Gaussian-like distribution to assign probabilities\n",
    "            emission_probabilities[i, j] = np.exp(-0.2 * (distance / (0.1 * (outputs - 1)))**2)\n",
    "\n",
    "    # Normalize probabilities for each state to sum to 1\n",
    "    emission_probabilities /= emission_probabilities.sum(axis=1, keepdims=True)\n",
    "\n",
    "    return emission_probabilities\n",
    "\n",
    "def generate_hmm_data(start_probabilities, transition_matrix, emission_probabilities, num_seq, seq_len, outputs):\n",
    "    \"\"\"\n",
    "    Generates sequences and hidden states from an HMM and formats them for machine learning tasks.\n",
    "\n",
    "    Parameters:\n",
    "        start_probabilities (array): Initial state probabilities.\n",
    "        transition_matrix (array): State transition probabilities.\n",
    "        emission_probabilities (array): Observation emission probabilities.\n",
    "        num_seq (int): Number of sequences to generate.\n",
    "        seq_len (int): Length of each sequence.\n",
    "        outputs (int): Number of possible output symbols (for one-hot encoding).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: One-hot encoded observation sequences. (Shape: num_seq, seq_len, outputs)\n",
    "        np.ndarray: Hidden state sequences.\n",
    "    \"\"\"\n",
    "    # Initialize HMM model\n",
    "    model = CategoricalHMM(n_components=len(start_probabilities))\n",
    "    model.startprob_ = start_probabilities\n",
    "    model.transmat_ = transition_matrix\n",
    "    model.emissionprob_ = emission_probabilities\n",
    "\n",
    "    # Generate sequences\n",
    "    sampled_sequences = np.zeros((num_seq, seq_len))\n",
    "    sampled_states = np.zeros((num_seq, seq_len))\n",
    "    for i in range(num_seq):\n",
    "        observations, hidden_states = model.sample(seq_len)\n",
    "        sampled_sequences[i] = observations.reshape((seq_len))\n",
    "        sampled_states[i] = hidden_states\n",
    "\n",
    "    # One-hot encode the observation sequences\n",
    "    one_hot_sequences = F.one_hot(torch.tensor(sampled_sequences).long(), num_classes=outputs)\n",
    "\n",
    "    return one_hot_sequences, sampled_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, trans, emiss = np.array([1,0]), generate_transition_matrix(2, 0.95), generate_emission_probabilities(2, 3)\n",
    "num_seq, seq_len, outputs = 5000, 100, 3\n",
    "seq, states = generate_hmm_data(start, trans, emiss, num_seq, seq_len, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0454\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(num_seq):\n",
    "    if states[i][1] == 1:\n",
    "        count += 1\n",
    "print(count/num_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1: [0.99, 0.01, 0.0]\n",
      "State 2: [0.8799999999999999, 0.01, 0.10999999999999999]\n",
      "State 3: [0.77, 0.01, 0.21999999999999997]\n",
      "State 4: [0.66, 0.01, 0.32999999999999996]\n",
      "State 5: [0.55, 0.01, 0.43999999999999995]\n",
      "State 6: [0.43999999999999995, 0.01, 0.55]\n",
      "State 7: [0.33, 0.01, 0.6599999999999999]\n",
      "State 8: [0.21999999999999997, 0.01, 0.77]\n",
      "State 9: [0.11000000000000004, 0.01, 0.8799999999999999]\n",
      "State 10: [0.0, 0.01, 0.99]\n"
     ]
    }
   ],
   "source": [
    "def build_hmm_emissions(num_states):\n",
    "    \"\"\"\n",
    "    Returns a list of emission probability vectors for an HMM\n",
    "    whose states linearly interpolate from [0.99, 0.01, 0]\n",
    "    to [0, 0.01, 0.99].\n",
    "\n",
    "    Mathematically, we define an interpolation factor α_i:\n",
    "\n",
    "        α_i = (i - 1) / (num_states - 1),   for i = 1, 2, ..., num_states\n",
    "\n",
    "    This ensures:\n",
    "      - At i = 1 (first state), α_1 = 0, so the probabilities are [0.99, 0.01, 0].\n",
    "      - At i = num_states (last state), α_M = 1, so the probabilities are [0, 0.01, 0.99].\n",
    "      - For 1 < i < M, α_i evenly interpolates between 0 and 1.\n",
    "\n",
    "    The emission probabilities for state i are then computed as:\n",
    "\n",
    "        p1(i) = 0.99 * (1 - α_i)    # Probability for output 1\n",
    "        p2(i) = 0.01                # Probability for output 2 (constant)\n",
    "        p3(i) = 0.99 * α_i          # Probability for output 3\n",
    "\n",
    "    Since p1(i) + p2(i) + p3(i) = 1 for all i, the probabilities are correctly normalized.\n",
    "\n",
    "    Arguments:\n",
    "    num_states -- integer, the total number of hidden states M\n",
    "\n",
    "    Returns:\n",
    "    emissions  -- list of lists, each being [p1, p2, p3]\n",
    "    \"\"\"\n",
    "    emissions = []\n",
    "    for i in range(1, num_states + 1):\n",
    "        alpha_i = (i - 1) / (num_states - 1)  # Interpolation factor\n",
    "        p1 = 0.99 * (1 - alpha_i)    # Probability for output 1\n",
    "        p2 = 0.01                    # Probability for output 2 (constant)\n",
    "        p3 = 0.99 * alpha_i          # Probability for output 3\n",
    "        emissions.append([p1, p2, p3])\n",
    "    \n",
    "    return emissions\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Suppose we want 5 states\n",
    "M = 10\n",
    "emission_probs = build_hmm_emissions(M)\n",
    "for state_index, probs in enumerate(emission_probs, start=1):\n",
    "    print(f\"State {state_index}: {probs}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
